# ==========================================
# END-TO-END HYBRID ASR CONFIGURATION (Raw Audio -> WavLM -> Decoder)
# ==========================================

# --- 1. EXPERIMENT INFO ---
experiment:
  project_name : "wavlm-hybrid-asr-v1"
  seed         : 42              
  output_dir   : "./logs/experiments"

# --- 2. DATA PIPELINE (Raw Audio + Manifest) ---
data:
  # Path diarahkan langsung ke file Colab yang sudah di-remap
  train_manifest  : "/content/data/processed/train_manifest_colab.jsonl"
  val_manifest    : "/content/data/processed/val_manifest_colab.jsonl"

  noise_dir       : "content/data/noise"
  
  sample_rate     : 16000
  
  # Batch Size DITURUNKAN dari 16 ke 8. 
  # Karena sekarang kita memuat raw audio & WavLM langsung ke GPU, butuh VRAM lebih besar.
  batch_size      : 32
  num_workers     : 8 
  max_duration    : 15.0

# --- 3. MODEL ARCHITECTURE (The Hybrid Core) ---
model:
  # The "Ears" (Acoustic Feature Extractor)
  backbone        : "microsoft/wavlm-base-plus"       # Ringan, cocok untuk eksperimen awal
  # backbone      : "microsoft/wavlm-large"           # Akurasi tinggi, butuh VRAM besar
  # backbone      : "facebook/wav2vec2-large-xlsr-53" # Sangat bagus untuk fonem IPA
  # backbone      : "facebook/hubert-large-ls960-ft"  # Sangat stabil
  freeze_backbone : False    # Set True di awal jika GPU Colab sering OOM
  
  # The "Brain" (Encoder)
  encoder_type    : "bilstm"
  input_dim       : 768      # Sesuai output WavLM Base
  hidden_dim      : 512
  num_enc_layers  : 3
  
  # The "Translator" (Decoder)
  decoder_hidden_dim : 512
  num_heads       : 8
  num_dec_layers  : 2
  
  # Regularisasi
  dropout         : 0.1

# --- 4. TRAINING HYPERPARAMETERS ---
train:
  epochs       : 20
  # Learning rate diturunkan sedikit (dari 3e-4 ke 1e-4) karena kita sekarang ikut men-training WavLM
  learning_rate: 1e-4     
  optimizer    : "adamw"  
  weight_decay : 0.005    
  
  # Kalibrasi Hybrid Loss (30% CTC, 70% Attention)
  ctc_weight   : 0.3

  fp16: True
  
  # Real Batch Size = batch_size (8) * accum_steps (4) = 32
  gradient_accumulation_steps: 4
  
  device       : "cuda"
  logging_steps : 100     
  save_total_limit: 2